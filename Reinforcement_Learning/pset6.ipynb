{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ekjsmds4_n2G",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem Set 6 (Total points: 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VxmzV7JUbHjx"
   },
   "source": [
    "### Q1. Markov Decision Process [20 points]\n",
    "Imagine an MDP corresponding to playing slot machines in a casino. Suppose you start with \\$20\\$ cash to spend in the casino,  and you decide to play until you lose all your money or until you double your money (i.e., increase your cash to at least \\$40\\$). There are two slot machines you can choose to play: 1) slot machine X costs \\$10\\$ to play and will pay out \\$20\\$ with probability \\$0.05\\$ and will pay \\$0\\$ otherwise;\n",
    "and \\$2)\\$ slot machine Y costs \\$20\\$ to play and will pay out \\$30\\$ with probability 0.01 and \\$0 \\$ otherwise. As you are playing, you keep choosing machine X or Y at each turn.\n",
    "\n",
    "Write down the MDP that corresponds to the above problem. Clearly specify the state space, action space, rewards and transition probabilities. Indicate which state(s) are terminal. Assume that the discount factor γ = 1.\n",
    "\n",
    "**Notes:** There are several valid ways to specify the MDP, so you have some flexibility in your solution. For example, rewards can take many different forms, but overall you should get a higher reward for stopping when you double your money than when you lose all your money!\n",
    "\n",
    "\n",
    "states: \n",
    "\n",
    "    {20 cash, 10 cash, 0 cash, 30 cash, 40 cash} The terminal states are 0 cash and 40 cash.\n",
    "\n",
    "action: \n",
    "\n",
    "    {play slot machine X, play slot machine Y}\n",
    "\n",
    "Transition: \n",
    "\n",
    "    P(20 cash, play slot X, 30 cash) = .05 #can play X, costs 10 but can win 20 => 30 cash next state\n",
    "    P(20 cash, play slot X, 10 cash) = .95 #can play X, costs 10 and not win    => 10 cash next state\n",
    "    P(20 cash, play slot Y, 30 cash) = .01 #can play Y, costs 20 but can win 30 => 30 cash next state\n",
    "    P(20 cash, play slot Y, 0 cash)  = .99 #can play Y, costs 20 and not win    => 0 cash: terminal state\n",
    "    \n",
    "    P(30 cash, play slot X, 40 cash) = .05 #can play X, costs 10 but can win 20 => 40 cash: terminal state\n",
    "    P(30 cash, play slot X, 20 cash) = .95 #can play X, costs 10 and not win    => 20 cash\n",
    "    P(30 cash, play slot Y, 40 cash) = .01 #can play Y, costs 20 but can win 30 => 40 cash: terminal state\n",
    "    P(30 cash, play slot Y, 10 cash) = .99 #can play Y, costs 20 and not win    => 10 cash:\n",
    "    \n",
    "    P(10 cash, play slot X, 20 cash) = .05 #can play X, costs 10 but can win 20 => 20 cash\n",
    "    P(10 cash, play slot X, 0  cash) = .95 #can play X, costs 10 and not win    => 0 cash: terminal state\n",
    "    \n",
    "Reward:\n",
    "    \n",
    "    R(20 cash, play slot X, 30 cash) = 0\n",
    "    R(20 cash, play slot X, 10 cash) = 0\n",
    "    R(20 cash, play slot Y, 30 cash) = 0\n",
    "    R(20 cash, play slot Y, 0 cash)  = -5\n",
    "    \n",
    "    R(30 cash, play slot X, 40 cash) = 10\n",
    "    R(30 cash, play slot X, 20 cash) = 0\n",
    "    R(30 cash, play slot Y, 40 cash) = 10\n",
    "    R(30 cash, play slot Y, 10 cash) = 0\n",
    "    \n",
    "    R(10 cash, play slot X, 20 cash) = 0\n",
    "    R(10 cash, play slot X, 0  cash) = -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M_H3z6wR_n2I"
   },
   "source": [
    "## Reinforcement Learning\n",
    "In the remainder of the problem set you will implement the Q-Learning Algorithm to solve the \"Frozen Lake\" problem.\n",
    "We​ ​will​ ​use​ ​OpenAI’s​ ​gym​ ​package​ ​to​ ​develop​ ​our​ ​solution​ ​in​ ​Python.​ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q8sgYT6oUKGz"
   },
   "source": [
    "### OpenAI Gym Setup \n",
    "​Read the​ ​set-up​ ​instructions for​ ​Gym​  [here](https://gym.openai.com/docs/).​ ​The​ ​instructions​ ​also​ ​give​ ​a​ ​good​ ​overview​ ​of​ ​the​ ​API​ ​for​ ​this​ ​package,​ ​so​ ​please​ ​read through​ ​it​ ​before​ ​proceeding.​\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pQ-qqj1n_n2K",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Frozen Lake\n",
    "Instead​ ​of​ ​using​ ​CartPole,​ ​we’re​ ​going​ ​to​ ​be​ ​using​ ​the​ [​​FrozenLake](https://gym.openai.com/envs/FrozenLake-v0/) environment. Read through the code for this environment by following the Github link.​ <br>\n",
    "\n",
    "Winter​ ​is​ ​quickly​ ​approaching,​ ​and​ ​we​ ​have​ ​to​ ​worry​ ​about​ ​navigating​ ​frozen​ ​lakes.​ ​It’s​ ​only early November,​ ​\n",
    "so​ ​the​ ​lakes​ ​haven’t​ ​completely​ ​frozen​ ​and​ ​if​ ​you​ ​make​ ​the​ ​wrong​ ​step​ ​you​ ​may​ ​fall​ ​through. \n",
    "We’ll​ ​need​ ​to​ ​learn​ ​how​ ​to​ ​get​ ​to​ ​our​ ​destination​ ​when​ ​stuck​ ​on​ ​the​ ​ice,​ ​without​ ​falling​ ​in.\n",
    "The​ ​lake​ ​we’re​ ​going​ ​to​ ​consider​ ​is a​ ​square​ ​lake​ ​with​ ​spots​ ​to​ ​step​ ​on​ ​in​ ​the​ ​shape​ ​of​ ​a​ ​grid.​ ​​<br>\n",
    "\n",
    "The surface is described using a 4x4 grid like the following\n",
    "\n",
    "        S F F F \n",
    "        F H F H\n",
    "        F F F H\n",
    "        H F F G\n",
    "\n",
    "​Each​ ​spot​ ​can have​ ​one​ ​of​ ​four​ ​states:\n",
    "- S:​ ​starting​ ​point.\n",
    "- G:​ ​goal​ ​point.\n",
    "- F:​ ​frozen​ ​spot,​ ​where​ ​it’s​ ​safe​ ​to​ ​walk.\n",
    "- H:​ ​hole​ ​in​ ​the​ ​ice,​ ​where​ ​it’s​ ​not​ ​safe​ ​to​ ​walk.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4fPxEMPQ_n2L"
   },
   "source": [
    "For example, consider the lake, <br>\n",
    "![alt text](https://cs-people.bu.edu/sunxm/frozen_lake_example.png)\n",
    "\n",
    "There are four possible actions: UP, DOWN, LEFT, RIGHT. Although we​ ​can​ ​see​ ​the​ ​path​ ​we​ ​need​ ​to​ ​walk,​ the agent does not. ​We’re​ ​going​ ​to​ ​train​ ​an​ ​agent​ ​to​ ​discover​ ​this​ ​via​ ​problem solving.​ ​However,​ ​walking​ ​on​ ​ice​ ​isn’t​ ​so​ ​easy!​ ​Sometimes​ ​you​ ​slip​ ​and​ ​aren’t​ ​able​ ​to​ ​take​ ​the​ ​step​ ​you intended.\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole.\n",
    "\n",
    "You receive a reward of 1 if you reach the goal, and zero otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ZlKYHn-XE2X"
   },
   "source": [
    "#### Q2. Walking on the Frozen Lake [10 points]\n",
    "\n",
    "Write a script that sets up the Frozen Lake environment and takes 10 walks through it, consisting of a maximum 10 randomly sampled actions during each walk. After each step, render the current state, and print out the reward and whether the walk is \"done\", i.e. in the terminal state, because the agent fell into a hole (stop if it is). In your own words, explain how this environment behaves. \n",
    "\n",
    "The environment is static and does not change whenever the agent takes an action. Only the state that the agent observes changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Using cached gym-0.17.3.tar.gz (1.6 MB)\n",
      "Requirement already satisfied: scipy in /Users/khoatran/opt/anaconda3/lib/python3.8/site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /Users/khoatran/opt/anaconda3/lib/python3.8/site-packages (from gym) (1.18.5)\n",
      "Collecting pyglet<=1.5.0,>=1.4.0\n",
      "  Using cached pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /Users/khoatran/opt/anaconda3/lib/python3.8/site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: future in /Users/khoatran/opt/anaconda3/lib/python3.8/site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654654 sha256=3656e67342c7828d8f45bb5e6f4a73af555ea98c77fa7f8f698fe3c44d584dbe\n",
      "  Stored in directory: /Users/khoatran/Library/Caches/pip/wheels/84/40/e7/14efb9870cfc92ac236d78cb721dce614ddec9666c8a5e0a35\n",
      "Successfully built gym\n",
      "Installing collected packages: pyglet, gym\n",
      "Successfully installed gym-0.17.3 pyglet-1.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: True\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: True\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: True\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: True\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Left)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Left)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Left)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: True\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: True\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Down)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Left)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: True\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: False\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0 Done: True\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('FrozenLake-v0').unwrapped\n",
    "env.reset()\n",
    "for _ in range(10):\n",
    "    env.reset()\n",
    "    for _ in range(10):\n",
    "        env.render()\n",
    "        observation, reward, done, info  = env.step(env.action_space.sample()) # take a random action\n",
    "        print(\"Reward:\",reward, \"Done:\", done)\n",
    "        if done:\n",
    "            break;\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CHPqSDaE_n2M"
   },
   "source": [
    "#### Q3. Q-Learning [50 points]\n",
    "\n",
    "You will ​implement​ ​Q-learning to solve the problem.​ Assume that the environment has states S and actions A. Use the ​function​ ​signature​ ​provided​ below:\n",
    "\n",
    "``` python\n",
    "def​ q_learning(env,​ ​alpha=0.5,​ gamma=0.95,​ ​epsilon=0.1, num_episodes=500):\n",
    "\"\"\" ​Performs​ ​Q-learning​ ​for​ ​the​ ​given​ ​environment.\n",
    "Initialize​ ​Q​ ​to​ ​all​ ​zeros.\n",
    ":param​ ​env:​ ​Unwrapped​ ​OpenAI​ ​gym​ ​environment.\n",
    ":param​ ​alpha:​ ​Learning​ ​rate​ ​parameter.\n",
    ":param​ ​gamma:​ ​Decay​ ​rate​ (future reward discount) ​parameter.\n",
    ":param​ ​num_episodes:​ ​Number​ ​of​ ​episodes​ ​to​ ​use​ ​for​ ​learning. \n",
    ":return:​ ​Q​ ​table, i.e. a table with the Q value for every <S, A> pair.\n",
    "\"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgFa7qID_n2O",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The pseudocode for Q-Learning was described in lecture, but for your reference, we provide it here:\n",
    "![alt text](https://cs-people.bu.edu/sunxm/q-learning.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, alpha=0.5,gamma=.95, epsilon=0.1,num_episodes=500, init_method = \"Z\", test = False):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    #init method Z: zeros  R: randoms\n",
    "    if init_method == \"Z\":\n",
    "        Q = np.zeros((env.nS, env.nA))\n",
    "    elif init_method == \"R\":\n",
    "        Q = np.random.rand(env.nS, env.nA)\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        done = False\n",
    "        \n",
    "        #reset state\n",
    "        currState = env.reset()\n",
    "        \n",
    "        #decrease exploration percentage\n",
    "        ep = epsilon\n",
    "        if ep < .01:\n",
    "            ep = .01\n",
    "        else:\n",
    "            ep = ep - .0001\n",
    "        while not done:\n",
    "            if not test:\n",
    "                #epsilon greedy\n",
    "                exploreProb = np.random.uniform(0,1)\n",
    "                if exploreProb < ep:\n",
    "                    currAction = env.action_space.sample()\n",
    "                else:\n",
    "                    currAction = np.argmax(Q[currState])\n",
    "            else:\n",
    "                #argmax\n",
    "                currAction = np.argmax(Q[currState])\n",
    "\n",
    "            #take step    \n",
    "            newState, reward, done, info= env.step(currAction)\n",
    "            \n",
    "            #max Q of new\n",
    "            max_Q_NS_NA = np.max(Q[newState])\n",
    "            \n",
    "            #update\n",
    "            newQ = Q[currState][currAction] + (alpha * ((reward + (gamma * max_Q_NS_NA) - Q[currState][currAction])))\n",
    "            Q[currState][currAction] = newQ\n",
    "            \n",
    "            #set curr state to new state\n",
    "            currState = newState\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1JHa8uv6_n2N"
   },
   "source": [
    "#### Q4. Main Function [20 points]\n",
    "You also need to implement the main function to solve the entire FrozenLake-v0 problem, including setting up the Gym environment, \n",
    "calling the q-learning function as defined above, printing out the returned Q-table, etc. <br>\n",
    "\n",
    "You should use the $\\epsilon$-greedy algorithm (solving Exploration-Exploitation Dilemma) to generate actions for each state. Try `num_episodes` with different values, e.g. `num_episodes=500, 1000, 5000, 10000, 50000`. You should also try different initializations for the Q-table, i.e. Random Initialization and Zero Initialization. \n",
    "\n",
    "Please do not change any default value of environment setting, such as `is_slippery`, if not mentioned above.\n",
    "\n",
    "Provide​ ​the​ ​final​ ​Q-table​ ​of​ ​​FrozenLake-v0​ for **each <num_episode, init_method>** you have tried [10 points],​ ​and analyze ​what​ you observe [10 points]. \n",
    "\n",
    "If you are interested in testing your learned Q-learned, you can compute the win rate (the probability of agents to goal following the Q-table). Note that you should use `argmax` instead of epsilon greedy to choose the action at the current state. \n",
    "Win Rate is a good way to test whether you write your algorithm correctly.\n",
    "\n",
    "Just for your reference, we test our implementation for 10000 trials after 50000-episode training using zero initialization and the win rate is 0.4985. You should achieve similar value under the same test.\n",
    "\n",
    "\n",
    "ANALYSIS: As the number of episodes increases, the Q table changes further. With the number of episodes at 50000, the final Q table has 0 for all actions for all the terminal states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of episodes: 500\n",
      "init method: R\n",
      "[[0.5504985  0.54046344 0.55015704 0.50969783]\n",
      " [0.5685941  0.50383225 0.47406929 0.47066274]\n",
      " [0.5424954  0.48269084 0.47941828 0.48140248]\n",
      " [0.49268248 0.46699931 0.47942985 0.18340451]\n",
      " [0.59005854 0.62856359 0.55894114 0.56375093]\n",
      " [0.61185289 0.13949386 0.29214465 0.36636184]\n",
      " [0.55604442 0.58417428 0.57758893 0.51423444]\n",
      " [0.59241457 0.04645041 0.60754485 0.17052412]\n",
      " [0.69174812 0.74610109 0.60894467 0.67292518]\n",
      " [0.67945776 0.75767774 0.60085268 0.49505697]\n",
      " [0.4765239  0.78901999 0.30146265 0.62024419]\n",
      " [0.25877998 0.66252228 0.31171108 0.52006802]\n",
      " [0.54671028 0.18485446 0.96958463 0.77513282]\n",
      " [0.88483645 0.60194558 0.70741418 1.06403675]\n",
      " [0.32268537 0.19598286 0.04522729 1.35206443]\n",
      " [0.38867729 0.27134903 0.82873751 0.35675333]]\n",
      "number of episodes: 500\n",
      "init method: Z\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "number of episodes: 1000\n",
      "init method: R\n",
      "[[0.50644482 0.5702656  0.51171312 0.46837302]\n",
      " [0.48534231 0.46878849 0.5194411  0.46408375]\n",
      " [0.46693305 0.46623345 0.53428926 0.46492327]\n",
      " [0.4597266  0.45459603 0.45886645 0.4621456 ]\n",
      " [0.54172166 0.63341978 0.5374195  0.54177959]\n",
      " [0.61185289 0.13949386 0.29214465 0.36636184]\n",
      " [0.45606998 0.57287079 0.34435014 0.53100679]\n",
      " [0.59241457 0.04645041 0.60754485 0.17052412]\n",
      " [0.6410392  0.63561013 0.79089643 0.63749734]\n",
      " [0.69357723 0.58341088 0.5892822  0.59666224]\n",
      " [0.58239996 0.67727719 0.03438852 0.54598549]\n",
      " [0.25877998 0.66252228 0.31171108 0.52006802]\n",
      " [0.54671028 0.18485446 0.96958463 0.77513282]\n",
      " [0.64598392 0.99467557 0.65601253 0.63832127]\n",
      " [0.0884925  0.53497754 0.04522729 1.31741169]\n",
      " [0.38867729 0.27134903 0.82873751 0.35675333]]\n",
      "number of episodes: 1000\n",
      "init method: Z\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "number of episodes: 5000\n",
      "init method: R\n",
      "[[0.55149886 0.49972364 0.58956048 0.51139369]\n",
      " [0.5019539  0.55037547 0.51137665 0.4884029 ]\n",
      " [0.49755446 0.49940424 0.56201374 0.48718521]\n",
      " [0.49948128 0.49786965 0.47708786 0.48720137]\n",
      " [0.60952613 0.59946972 0.55395699 0.54418431]\n",
      " [0.61185289 0.13949386 0.29214465 0.36636184]\n",
      " [0.54576453 0.59215651 0.57336714 0.55281758]\n",
      " [0.59241457 0.04645041 0.60754485 0.17052412]\n",
      " [0.74844574 0.89733871 0.75182424 0.76541   ]\n",
      " [0.66823128 0.79257804 0.7366075  0.70694899]\n",
      " [0.72597838 0.66738328 0.68268606 0.66080876]\n",
      " [0.25877998 0.66252228 0.31171108 0.52006802]\n",
      " [0.54671028 0.18485446 0.96958463 0.77513282]\n",
      " [0.92422003 1.0530922  0.95290452 0.90753825]\n",
      " [1.04055451 1.06960608 1.022507   1.02052069]\n",
      " [0.38867729 0.27134903 0.82873751 0.35675333]]\n",
      "number of episodes: 5000\n",
      "init method: Z\n",
      "[[2.44776422e-01 1.58550390e-01 1.46705311e-01 1.45451936e-01]\n",
      " [3.62909384e-02 3.05865793e-02 8.42972829e-02 1.54388567e-01]\n",
      " [7.18724342e-02 1.06183150e-01 6.39533410e-02 9.37587624e-02]\n",
      " [8.08323891e-02 3.24316328e-02 3.37429735e-02 1.04562198e-01]\n",
      " [3.95157596e-01 7.33770128e-02 1.10291097e-01 9.99638558e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.60933576e-04 9.23698514e-03 3.55434595e-02 1.74299107e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.21882348e-02 1.88176798e-01 2.10721882e-01 5.65585828e-01]\n",
      " [1.85117678e-01 5.71450154e-01 4.45026648e-01 8.67083240e-02]\n",
      " [6.80686521e-01 3.11099023e-01 9.99508053e-02 1.40278735e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [4.52113168e-01 9.92110944e-02 6.79193944e-01 4.49637599e-01]\n",
      " [4.35912359e-01 9.19237974e-01 5.90198884e-01 5.71196276e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
      "number of episodes: 10000\n",
      "init method: R\n",
      "[[0.56393719 0.5362209  0.5392814  0.51587622]\n",
      " [0.4785093  0.50874918 0.4899624  0.48027539]\n",
      " [0.48173776 0.47801956 0.48082529 0.4847611 ]\n",
      " [0.48704177 0.48931278 0.55063022 0.48328213]\n",
      " [0.56435015 0.72112978 0.57993346 0.56784936]\n",
      " [0.61185289 0.13949386 0.29214465 0.36636184]\n",
      " [0.60923541 0.63277387 0.56819633 0.54382926]\n",
      " [0.59241457 0.04645041 0.60754485 0.17052412]\n",
      " [0.74031945 0.90369627 0.77050524 0.77274471]\n",
      " [0.75013494 0.89500493 0.58484675 0.72123062]\n",
      " [0.61460671 0.73549539 0.6346824  0.70378371]\n",
      " [0.25877998 0.66252228 0.31171108 0.52006802]\n",
      " [0.54671028 0.18485446 0.96958463 0.77513282]\n",
      " [0.92268756 0.9004001  0.8053061  0.89926124]\n",
      " [0.95960485 1.00098089 1.00206902 1.5957106 ]\n",
      " [0.38867729 0.27134903 0.82873751 0.35675333]]\n",
      "number of episodes: 10000\n",
      "init method: Z\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "number of episodes: 50000\n",
      "init method: R\n",
      "[[0.52598751 0.50469776 0.56237737 0.49535942]\n",
      " [0.50148506 0.51848984 0.57081149 0.52288758]\n",
      " [0.53477244 0.56981634 0.48116549 0.51502685]\n",
      " [0.55462839 0.48119513 0.47841893 0.4853321 ]\n",
      " [0.54911511 0.56399926 0.7394725  0.55069456]\n",
      " [0.61185289 0.13949386 0.29214465 0.36636184]\n",
      " [0.56523872 0.60744553 0.61977656 0.56455847]\n",
      " [0.59241457 0.04645041 0.60754485 0.17052412]\n",
      " [0.74635231 0.87628864 0.69598811 0.76433875]\n",
      " [0.62327062 0.93606169 0.78117807 0.67838585]\n",
      " [0.60595169 0.73565028 0.65804726 0.65856106]\n",
      " [0.25877998 0.66252228 0.31171108 0.52006802]\n",
      " [0.54671028 0.18485446 0.96958463 0.77513282]\n",
      " [0.90304095 0.92992973 1.24833157 0.90462978]\n",
      " [1.08943457 1.34291473 1.15088582 1.15656326]\n",
      " [0.38867729 0.27134903 0.82873751 0.35675333]]\n",
      "number of episodes: 50000\n",
      "init method: Z\n",
      "[[0.14131438 0.12029907 0.12893783 0.13076668]\n",
      " [0.080137   0.07158801 0.031121   0.1193003 ]\n",
      " [0.05906959 0.06063423 0.06290004 0.07302855]\n",
      " [0.04793487 0.0245233  0.01004081 0.07275349]\n",
      " [0.22076694 0.06297318 0.06436951 0.03814453]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.01132289 0.00589803 0.10605686 0.00887796]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.06385461 0.05896702 0.15870151 0.34743059]\n",
      " [0.14071514 0.35052146 0.2638124  0.19904682]\n",
      " [0.21981362 0.1790614  0.14767863 0.08189078]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.16346778 0.36844208 0.42033706 0.33033451]\n",
      " [0.40605292 0.45379595 0.35594373 0.47509721]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "numE = [500, 1000, 5000, 10000, 50000]\n",
    "initMethod = [\"R\", \"Z\"]\n",
    "\n",
    "env = gym.make('FrozenLake-v0').unwrapped\n",
    "\n",
    "for numEps in numE:\n",
    "    for method in initMethod:\n",
    "        print(\"number of episodes:\", numEps)\n",
    "        print(\"init method:\", method)\n",
    "        print(q_learning(env, .5, .95, .1, numEps, method, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FtF6U2vg_n2O"
   },
   "source": [
    "#### Additional Instructions\n",
    "If​ ​you’re​ ​new​ ​to​ OpenAI’s​ ​Gym,​ first ​go​ ​through​ ​OpenAI’s​ ​full​ ​tutorial listed​ ​earlier​ ​and​ ​visit​ ​the​ ​Appendix​ ​to​ ​this​ ​homework​ ​before​ ​proceeding.\n",
    "Some additional rules:\n",
    "- Only submit **original code** written entirely by you.\n",
    "- **Permitted​​ non-standard ​​libraries**:​​ ​gym​,​​ ​numpy.\n",
    "- **Only​ ​use​ ​numpy​ ​for​ ​random​ ​sampling,​ ​and​ ​seed​ ​at​ ​the​ ​beginning​ ​of​ ​each​ ​function​ ​with**:\n",
    "np.random.seed(42)\n",
    "- **Unwrap​ ​the​ ​OpenAI​ ​gym​ ​before​ ​providing​ ​them​ ​to​ ​these​ ​functions.** <br>\n",
    "env​ ​=​ ​gym.make(“FrozenLake-v0”).unwrapped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RjpJpTbV_n2Q"
   },
   "source": [
    "## Appendix\n",
    "\n",
    "This​ ​appendix​ ​includes​ ​references​ ​to​ ​APIs​ ​you​ ​may​ ​find​ ​useful.​ ​If​ ​the​ ​description​ ​sounds​ ​useful,​ ​check out\n",
    "the​ ​respective​ ​package’s​ ​documentation​ ​for​ ​a​ ​much​ ​better​ ​description​ ​than​ ​we​ ​could​ ​provide.\n",
    "\n",
    "#### Numpy\n",
    "- np.zeros:​ ​N-dimensional​ ​tensor​ ​initialized​ ​to​ ​all​ ​0s.\n",
    "- np.ones:​ ​N-dimensional​ ​tensor​ ​initialized​ ​to​ ​all​ ​1s.\n",
    "- np.eye:​ ​N-dimensional​ ​tensor​ ​initialized​ ​to​ ​a​ ​diagonal​ ​matrix​ ​of​ ​1s.\n",
    "- np.random.choice:​ ​Randomly​ ​sample​ ​from​ ​a​ ​list,​ ​allowing​ ​you​ ​to​ ​specify​ ​weights.\n",
    "- np.argmax:​ ​Index​ ​of​ ​the​ ​maximum​ ​element.\n",
    "- np.abs:​ ​Absolute​ ​value.\n",
    "- np.mean:​ ​Average​ ​across​ ​dimensions.\n",
    "- np.sum:​ ​Sum​ ​across​ ​dimensions.\n",
    "\n",
    "### OpenAI Gym\n",
    "- Environment​ ​(unwrapped):<br>\n",
    "env.nS #​ ​Number​ ​of​ ​spaces. <br>\n",
    "env.nA #​ ​Number​ ​of​ ​actions. <br>\n",
    "env.P #​ ​Dynamics​ ​model.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pset7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
